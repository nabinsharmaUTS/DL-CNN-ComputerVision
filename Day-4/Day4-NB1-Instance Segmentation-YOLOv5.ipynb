{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"hide_input":false,"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"toc":{"nav_menu":{"height":"122px","width":"252px"},"number_sections":true,"sideBar":true,"skip_h1_title":false,"toc_cell":false,"toc_position":{"height":"758px","left":"0px","right":"1096px","top":"73px","width":"253px"},"toc_section_display":"block","toc_window_display":true},"colab":{"provenance":[],"collapsed_sections":["FOYg8WSFZ0Jl","K_dpwUYqIj5L","hoVEDxalH2Ly"]},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"FOYg8WSFZ0Jl"},"source":["# **Deep Learning and CNN for Computer Vision, Hokkaido University**\n","\n","## Day 4, Notebook -1: Instance Sgmentation using YoloV5.\n","\n","In this part of the session you will be exploring how to use the YOLO v5 for instance segmentation\n","\n","\n","So lets get started!\n","\n","\n","We will use the https://github.com/ultralytics/yolov5 repository for this tutorial\n","\n","Reference: https://colab.research.google.com/github/ultralytics/yolov5/blob/master/segment/tutorial.ipynb"]},{"cell_type":"markdown","metadata":{"id":"YRfPs8W7JXa5"},"source":["# Step 1: Clone github repository"]},{"cell_type":"markdown","source":["#### **Connect to Google Drive**"],"metadata":{"id":"JnCIG7zCFt_i"}},{"cell_type":"code","metadata":{"id":"fe8Pk-N-TggN"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Custom Dataset Setup (We will use Toys dataset)**\n","\n"],"metadata":{"id":"9WtcnZSZG0NY"}},{"cell_type":"code","source":["# Change path to you google drive location of the dataset as needed\n","\n","## ENTER PATH TO DATASET ZIP FILE HERE ##\n","!unzip '/content/gdrive/MyDrive/Hokkaido Uni DL/Day-4/toys_yolov5.zip'"],"metadata":{"id":"wCaDXvLZozjZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Clone the Github Repo (Default location /content/)**"],"metadata":{"id":"rS7VwEhDGCqV"}},{"cell_type":"code","source":["# By Default Cloned under /content/\n","!git clone https://github.com/ultralytics/yolov5"],"metadata":{"id":"72m3jBaxjb_a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Install the required libraries"],"metadata":{"id":"b1o4IcqcGZpc"}},{"cell_type":"code","source":["%cd yolov5\n","from yolov5 import utils\n","#display = utils.notebook_init()  # checks Pytorch and GPU used\n","!pip install -r requirements.txt"],"metadata":{"id":"ZSxIReiajdY3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Quick Reference on YAML**"],"metadata":{"id":"K_dpwUYqIj5L"}},{"cell_type":"markdown","source":["#### [**What is YAML?**](https://circleci.com/blog/what-is-yaml-a-beginner-s-guide/)\n","\n","YAML --> **Y**et **A**nother **M**arkup **L**anguage\n","\n","YAML is a digestible data serialization language often used to create configuration files with any programming language.\n","\n","Designed for human interaction, YAML is a strict superset of JSON, another data serialization language. But because itâ€™s a strict superset, it can do everything that JSON can and more. One major difference is that newlines and indentation actually mean something in YAML, as opposed to JSON, which uses brackets and braces.\n","\n","Reference and Source: https://circleci.com/blog/what-is-yaml-a-beginner-s-guide/"],"metadata":{"id":"hoVEDxalH2Ly"}},{"cell_type":"markdown","source":["# Step 2: Setup Training Configuration"],"metadata":{"id":"EPPXGoraGnYk"}},{"cell_type":"code","source":["#@title Setup Training YAML File\n","number_of_classes = 16 #@param {type:\"integer\"}\n","with open('new_train_yaml', 'w+') as file:\n","    file.write(\n","        f\"\"\"\n","            # YOLOv5 ðŸš€ by Ultralytics, AGPL-3.0 license\n","\n","            # Parameters\n","            nc: {number_of_classes}  # number of classes\n","            depth_multiple: 0.33  # model depth multiple\n","            width_multiple: 0.5  # layer channel multiple\n","            anchors:\n","              - [10,13, 16,30, 33,23]  # P3/8\n","              - [30,61, 62,45, 59,119]  # P4/16\n","              - [116,90, 156,198, 373,326]  # P5/32\n","\n","            # YOLOv5 v6.0 backbone\n","            backbone:\n","              # [from, number, module, args]\n","              [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n","              [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n","              [-1, 3, C3, [128]],\n","              [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n","              [-1, 6, C3, [256]],\n","              [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n","              [-1, 9, C3, [512]],\n","              [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n","              [-1, 3, C3, [1024]],\n","              [-1, 1, SPPF, [1024, 5]],  # 9\n","              ]\n","\n","            # YOLOv5 v6.0 head\n","            head:\n","              [[-1, 1, Conv, [512, 1, 1]],\n","              [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","              [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n","              [-1, 3, C3, [512, False]],  # 13\n","\n","              [-1, 1, Conv, [256, 1, 1]],\n","              [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","              [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n","              [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n","\n","              [-1, 1, Conv, [256, 3, 2]],\n","              [[-1, 14], 1, Concat, [1]],  # cat head P4\n","              [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n","\n","              [-1, 1, Conv, [512, 3, 2]],\n","              [[-1, 10], 1, Concat, [1]],  # cat head P5\n","              [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n","\n","              [[17, 20, 23], 1, Segment, [nc, anchors, 32, 256]],  # Detect(P3, P4, P5)\n","              ]\n","        \"\"\")"],"metadata":{"id":"LGjHqY0LjgOO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 3: Setup Dataset paths"],"metadata":{"id":"UpE8cKOjHNr6"}},{"cell_type":"code","source":["#@title Setup Dataset Configuration (Data.yaml)\n","train_data_dir = \"/content/toys_yolov5/train\" #@param {type:\"string\"}\n","val_data_dir = \"/content/toys_yolov5/valid\" #@param {type:\"string\"}\n","class_names = ['banana', 'chain', 'cleanser', 'cracker_box', 'flat_screw', 'fork', 'glass_cleaner', 'grass_cleaner', 'knife', 'mug', 'mustard', 'pen', 'plus_screw', 'rubick_cube', 'spoon', 'tennis_ball'] #@param {type:\"raw\"}\n","with open('new_data_yaml', 'w+') as file:\n","    file.write(\n","        f\"\"\"\n","        train: {train_data_dir}\n","        val: {val_data_dir}\n","\n","        nc: {number_of_classes}\n","        names: {class_names}\n","        \"\"\"\n","    )"],"metadata":{"id":"3wuviSSijmUq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3oKtE4g-aNn"},"outputs":[],"source":["#@title Select YOLOv5 ðŸš€ logger {run: 'auto'}\n","logger = 'TensorBoard' #@param ['TensorBoard', 'Comet', 'ClearML']\n","\n","if logger == 'TensorBoard':\n","  %load_ext tensorboard\n","  %tensorboard --logdir /content/yolov5/runs/train-seg/\n","elif logger == 'Comet':\n","  %pip install -q comet_ml\n","  import comet_ml; comet_ml.init()\n","elif logger == 'ClearML':\n","  import clearml; clearml.browser_login()"]},{"cell_type":"markdown","source":["# Step 4: Start Training"],"metadata":{"id":"w0inZ8cpIyJg"}},{"cell_type":"markdown","source":["**Configuation to try:**\n","\n","*   **Image Size**: 416\n","*   **Batch Size**: 16\n","*   **Epochs**: 200\n","*   **Data Source details** : new_data_yaml (Created earlier)\n","*   **Training details** : new_train_yaml (Created earlier)\n","\n","Example:\n","```shell\n","!python /content/yolov5/segment/train.py --img image_size --batch batch_size --epochs epochs --data /path/to/data/yml --cfg /path/to/train/yml\n","```"],"metadata":{"id":"5EaomAjuI4Ab"}},{"cell_type":"code","source":["## ENTER CODE TO START TRAINING ##\n","!python /content/yolov5/segment/train.py --img 416 --batch 16 --epochs 50 --data /content/yolov5/new_data_yaml --cfg /content/yolov5/new_train_yaml\n"],"metadata":{"id":"XYjrtDp-jpyH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 5: Test the trained model on sample images"],"metadata":{"id":"ClPQD4IIKCce"}},{"cell_type":"markdown","source":["### **Validation**"],"metadata":{"id":"1xo6EDjHFn9V"}},{"cell_type":"markdown","source":["**Configuation to try:**\n","\n","*   **Image Size**: 416\n","*   **Data Source details** : new_data_yaml (Created earlier)\n","\n","Example:\n","```shell\n","!python /content/yolov5/segment/val.py --weights path/to/weithgs/best.pt --data path/to/data/yaml --img 416\n","```"],"metadata":{"id":"KrPkBzZGIBY2"}},{"cell_type":"code","source":["!python /content/yolov5/segment/val.py --weights /content/yolov5/runs/train-seg/exp/weights/best.pt --data /content/yolov5/new_data_yaml --img 416"],"metadata":{"id":"Q6gscgmDFuN2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Inference/Testing**\n","'detect.py' is used to run YOLOv5 testing/inference on different types of inputs such as: image, video, webcam input, directory glob, Youtube, RTSP/RTMP/HTTP Stream.\n","\n","**Usage Syntax:**\n","\n","```shell\n","python segment/predict.py --source 0  # webcam\n","                             img.jpg  # image\n","                             vid.mp4  # video\n","                             screen  # screenshot\n","                             path/  # directory\n","                             'path/*.jpg'  # glob\n","                             'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n","                             'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n","                          --weights path/to/weights\n","                          --img 416\n","                          --conf 0.5\n","                          --save-txt\n","```\n","Example:\n","```shell\n","!python /content/yolov5/segment/predict.py --source '/path/to/img' --weights 'path/to/weights/best.pt' --img image_size --conf confidence_threshold --save-txt\n","```"],"metadata":{"id":"-XE_BpBbXp8p"}},{"cell_type":"code","source":["## ENTER CODE TO START INFERENCE ON THE IMAGES\n","!python /content/yolov5/segment/predict.py --source '/content/toys_yolov5/test/images/100017.jpg' --weights '/content/yolov5/runs/train-seg/exp/weights/best.pt' --img 416 --conf 0.4 --save-txt\n"],"metadata":{"id":"g1yytrQyUUnU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 6: Display result images"],"metadata":{"id":"JuUhFdRfY_-O"}},{"cell_type":"code","source":["import cv2\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","\n","# This is needed to display the images.\n","%matplotlib inline"],"metadata":{"id":"vFUOsFxTT8N-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = Image.open('/content/yolov5/runs/predict-seg/exp/100017.jpg')\n","plt.imshow(image)"],"metadata":{"id":"ggSen4jYXLdt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 7: Display performance analysis"],"metadata":{"id":"ASzqmy_GAXxt"}},{"cell_type":"markdown","source":["Training results are automatically logged to Tensorboard and CSV as `results.csv`, which is plotted as `results.png` (below) after training completes. You can also plot any `results.csv` file manually:"],"metadata":{"id":"z79oz5iPbUqB"}},{"cell_type":"code","source":["image = Image.open('/content/yolov5/runs/train-seg/exp/results.png') # Change 'exp' to the last in the train directory\n","plt.imshow(image)"],"metadata":{"id":"b4xEYywVFB8f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#[OPTIONAL] Alternate way of ploting the curves from CSV file\n","from utils.plots import plot_results\n","# Change the path to the last exp under train folder.\n","plot_results('/content/yolov5/runs/train-seg/exp/results.csv')  # plot 'results.csv' as 'results.png'"],"metadata":{"id":"xrwRiD1RAqEJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 8: Optional (Video/Real-Time Inference)"],"metadata":{"id":"NvOIo51GIvHg"}},{"cell_type":"markdown","source":["### Try on a Road Traffic video with pre-trained weights"],"metadata":{"id":"PN0GcVTUaQ7w"}},{"cell_type":"code","source":["!python /content/yolov5/segment/predict.py --source '/content/gdrive/MyDrive/Hokkaido Uni DL/Day-4/Road_Traffic.mp4'"],"metadata":{"id":"WjNTdsU2ZJVE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert mp4 to webm\n","!ffmpeg -i /content/yolov5/runs/predict-seg/exp2/Road_Traffic.mp4 -vcodec vp9 ./DetectionResult.webm"],"metadata":{"id":"StVVfnJvfmqw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the Result!!\n","import io\n","from base64 import b64encode\n","from IPython.display import HTML\n","\n","with  io.open('/content/yolov5/DetectionResult.webm','r+b') as f:\n","    mp4 = f.read()\n","data_url = \"data:video/webm;base64,\" + b64encode(mp4).decode()\n","HTML(\"\"\"\n","<video width=800 controls>\n","      <source src=\"%s\" type=\"video/webm\">\n","</video>\n","\"\"\" % data_url)"],"metadata":{"id":"4BWc_ff-fJlF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PdCwkoyVnW4r"},"execution_count":null,"outputs":[]}]}