{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0FslCE7TeuuQ"},"source":["# **Deep Learning and CNN for Computer Vision, Hokkaido University**\n","\n","## Day 2, Notebook -1: Image CLassification using CNN\n","\n","\n","In this session you will be implementing Convolutional Neural Network for Fashion MNIST dataset classification.\n","\n","So lets get started!\n","\n","## Tasks for this notebook:\n","\n","1. Implementation of a CNN for Dogs and Cats classification using Keras API.\n","2. Train and test model\n"]},{"cell_type":"markdown","metadata":{"id":"rahZjYapfk0y"},"source":["### Step 1: Import required packages\n","\n","We will need tensorflow, numpy, os and keras\n"]},{"cell_type":"code","metadata":{"id":"OB0ONDu2OVEm"},"source":["import tensorflow as tf\n","import os\n","import numpy as np\n","import math, numpy as np\n","import sklearn.datasets\n","import matplotlib.pyplot as plt\n","import h5py\n","import glob\n","import cv2\n","import keras.utils as image\n","from tensorflow import keras"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g1gJI8OsOuZQ"},"source":["### Step 2: Download the Fashion Mnist dataset using keras"]},{"cell_type":"code","metadata":{"id":"QsFlFK_hOyqt"},"source":["fashionMnist=tf.keras.datasets.fashion_mnist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KhqsWWm2O6Hf"},"source":["# Load data from fashion mnist dataset using the load_data() method.\n","(train_images, train_labels), (test_images, test_labels) = fashionMnist.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hNZ63cL7PuzM"},"source":["# Display the shapes of the training images\n","print(train_images.shape)\n","print(train_images.dtype)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FOQlTPcTkqyX"},"source":["#define the class names for the fashion mnist dataset\n","class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Qh2YwnckTbm"},"source":["## Display an image from the dataset\n","import matplotlib.pyplot as plt\n","plt.imshow(train_images[3])\n","print(train_labels[3])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DpeklHZfQRAG"},"source":["**Note :** Scikit-learn import the Fashion MNIST dataset as a 1-D array while Keras API load the dataset in 28X28 format."]},{"cell_type":"markdown","metadata":{"id":"pVNEvf6-PQIQ"},"source":["### Step 3: Normalize the dataset and split a small part of the training set into validation set\n","\n","\n","- Validation set: first 5000 samples (total 5000 samples)\n","- Training set: 5000 to remaining (total 55000 samples)"]},{"cell_type":"code","metadata":{"id":"ny-1XI3QSbav"},"source":["## Using slicing to split the training to train and validation\n","\n","train_images=train_images.reshape(60000, 28, 28, 1)\n","valid_images= train_images[:5000] / 255.0\n","valid_labels = train_labels[:5000]\n","\n","train_images  = train_images[5000:] / 255.0\n","train_labels=train_labels[5000:]\n","\n","test_images = test_images.reshape(10000, 28, 28, 1)\n","test_images = test_images / 255.0\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLJRDrr-dx6i"},"source":["# Print the shapes for Train, Validation, and Test dataset.\n","print(np.shape(train_images))\n","print(np.shape(valid_images))\n","print(np.shape(test_images))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rCzF5ilprJdt"},"source":["[**Expected** Output]\n","\n","(55000, 28, 28)\n","(5000, 28, 28)\n","(10000, 28, 28)"]},{"cell_type":"markdown","metadata":{"id":"Qp3nej13Pa0G"},"source":["### Step 4:  Design the CNN Architecture\n","\n","Design the following CNN architecture:\n","\n","<img src='http://drive.google.com/uc?export=view&id=1KBmj460idGx6mWbAKsH1bsEYmjpthdPB' alt='Conv'>\n","\n","\n","Input: $64 X 64 X 3$ image\n","\n","Activation function in CONV layer: Relu\n","\n","Activation function in Output layer : softmax, 10 classes\n","\n","**Hint:** Use Conv2D(), MaxPooling2D(), Flatten(), and Dense()\n","\n"]},{"cell_type":"code","metadata":{"id":"DMYDkJhtPeeg"},"source":["model = tf.keras.models.Sequential([\n","  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n","  tf.keras.layers.MaxPooling2D(2, 2),\n","  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","  tf.keras.layers.MaxPooling2D(2,2),\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dense(10, activation='softmax')\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A5OiZgAUT9l6"},"source":["## **Notes:**\n","* **Sequential model.** This is the simplest kind of Keras model, for neural networks which defines a SEQUENCE of layers.\n","\n","* **Flatten.** Flatten just takes that image and turns it into a 1-dimensional vector.\n","\n","* Next we add a second Dense hidden layer with 128 neurons, also using the ReLU activation function.  **Dense.** Add a layer to the neural network which is followed by activation function of ReLU. The ReLU only passes the value greater than 0 and for all other values of X it passes 0.\n","e.g. If X>0 return X, else return 0\"\n","\n","* Finally, we add a Dense output layer with 10 neurons (one per class), using the softmax activation function.\n","\n","* **Softmax** The softmax takes a set of values and select the biggest one from the set of values."]},{"cell_type":"markdown","metadata":{"id":"fPG3kxXyPsIv"},"source":["## Step 5: Training the model"]},{"cell_type":"markdown","metadata":{"id":"hpJB0lyhqKr5"},"source":["**\"sparse_categorical_crossentropy\":**   The dataset contains sparse labels and the classes are exclusive.\n","\n","**One-hot vector encoding** This is sometime used for encoding the labels if there one target  probability per class for each instance. For example.\n","[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.] represent one-hot encoding for class 4. In such case, **\"categorical_crossentropy\"** loss is used.\n","\n","**\"sigmoid_crossentropy\"** This loss is used for binary class classification problems and also **\"sigmoid\"** activation function is used instead of Softmax.\n","\n"]},{"cell_type":"code","source":["# Using Plot_Model from Keras.Utils\n","model_img_file = 'CNN-model1.png'\n","tf.keras.utils.plot_model(model, to_file=model_img_file,\n","                          show_shapes=True,\n","                          show_layer_activations=False,\n","                          show_dtype=False,\n","                          show_layer_names=False )\n","\n","# Also Try:\n","# show_shapes=True, show_layer_activations=True, show_dtype=True, show_layer_names=True"],"metadata":{"id":"zP2AxgizGjyT"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jBzPoV12PuNt"},"source":["# Compile the model and start training the model\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.summary()\n","model.fit(train_images, train_labels, epochs=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z_-DVZd9W6EW"},"source":["# Process the test images and find the accuracy\n","test_loss = model.evaluate(test_images, test_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z62h-9uyQmsQ"},"source":["### Summary of the model"]},{"cell_type":"code","metadata":{"id":"TjqZ2vh0QogG"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BsHuvS44ag6a"},"source":["## Step 6: Evaluation on test dataset"]},{"cell_type":"code","metadata":{"id":"E4pDW0FsUUmI"},"source":["model.evaluate(test_images, test_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cL4ddMJ-_hy-"},"source":["## Task: Image classification using Cats and Dogs Dataset."]},{"cell_type":"markdown","metadata":{"id":"dSZZuSdDtJWl"},"source":["###  Step: 1 Mount the Google Drive to access the Cats and Dogs Dataset\n","Reference: https://github.com/ardamavi/Dog-Cat-Classifier\n","\n"]},{"cell_type":"code","metadata":{"id":"wkhugCRrJUgB"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UFiaj8HACr9Q"},"source":["cd /content/gdrive/MyDrive/Hokkaido Uni DL/Day-2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k-mV5ocACv2j"},"source":["!unzip Cats-Dogs-dataset-64.zip\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6HQ8CEEmKTKv"},"source":["### Step : 2 Image Generators: (Preparing the dataset for train, validation and testing)\n","\n","In Keras  **keras.preprocessing.image.ImageDataGenerator** class  can be used to read images and extract labels from them via .flow_from_directory. The image generator can also be used for data augmentation. The image generators can used easily with Keras model that accept data generators as inputs. such as fit_generator, evaluate_generator, and predict_generator.\n"]},{"cell_type":"code","metadata":{"id":"m3IG-LSyV3SI"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# All images will be rescaled by 1./255\n","train_datagen = ImageDataGenerator(rescale=1/255)\n","validation_datagen = ImageDataGenerator(rescale=1/255)\n","\n","\n","# Flow training images in batches of 32 using train_datagen generator\n","train_generator = train_datagen.flow_from_directory(\n","        '/content/gdrive/MyDrive/Hokkaido Uni DL/Day-2',  # This is the source directory for training images\n","        target_size=(64, 64),  # All images will be resized to 64X64\n","        batch_size=30,\n","        # Since we use binary_crossentropy loss, we need binary labels\n","        class_mode='binary')\n","\n","# Flow training images in batches of 32 using train_datagen generator\n","validation_generator = validation_datagen.flow_from_directory(\n","        '/content/gdrive/MyDrive/Hokkaido Uni DL/Day-2',  # This is the source directory for training images\n","        target_size=(64, 64),  # All images will be resized to 64X64\n","        batch_size=30,\n","        # Since we use binary_crossentropy loss, we need binary labels\n","        class_mode='binary')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"obfTS5wsLrYr"},"source":["### Step 3: Create the CNN model:\n","\n","Create the following CNN model:\n","\n","<img src='http://drive.google.com/uc?export=view&id=1EAWFwp7T92q3Lm1ZrX9A2-wnvhfAfzSF' alt='Conv'>\n","\n","Input: $64 X 64 X 3$ image\n","\n","Activation function in CONV layer: Relu\n","\n","Activation function in Output layer : sigmoid, 2 classes\n","\n","**Hint:** Use Conv2D(), MaxPooling2D(), Flatten(), and Dense()"]},{"cell_type":"code","metadata":{"id":"uAWjv88rC9F4"},"source":["## WRITE YOUR CODE HERE ## (~11 lines)\n","model1 = tf.keras.models.Sequential([\n","    # This is the first convolution\n","    tf.keras.layers.Conv2D(??),\n","    tf.keras.layers.MaxPooling2D(??),\n","    # The second convolution\n","\n","    # The third convolution\n","\n","    # The fourth convolution\n","\n","    # Flatten the results to feed into a DNN\n","\n","    # 512 neuron hidden layer\n","\n","    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n","\n","    ])\n","\n","## END YOUR CODE HERE ##"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_vi2btHwGwBb"},"source":["##Print the model summary\n","model1.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Using Plot_Model from Keras.Utils\n","model_img_file = 'CNN-model2.png'\n","tf.keras.utils.plot_model(model1, to_file=model_img_file,\n","                          show_shapes=True,\n","                          show_layer_activations=False,\n","                          show_dtype=False,\n","                          show_layer_names=False )\n","\n","# Also Try:\n","# show_shapes=True, show_layer_activations=True, show_dtype=True, show_layer_names=True"],"metadata":{"id":"hTVIxT2-FaFc"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2T_v_VO6GxIx"},"source":["## Compile the model and add loss, optimizer and metrics\n","## WRITE YOUR CODE HERE ## (~1 line)\n","model1.compile(loss=??,\n","              optimizer=tf.optimizers.Adam(),\n","              metrics=??)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1qu6vHJ6YXEi"},"source":["# Train/fit the model using the training and validation set.\n","history = model1.fit(\n","      train_generator,\n","      steps_per_epoch=8,\n","      epochs=15,\n","      verbose=1,\n","      validation_data = validation_generator,\n","      validation_steps=8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c0vYkTLGHeUg"},"source":["## Plot the Training and Validation loss\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(len(loss))\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'r', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j4IBgYCYooGD"},"source":["### Clearing all the resources\n","\n","Terminate the kernel and free memory resources"]},{"cell_type":"code","metadata":{"id":"iLzc4iQ6mchi"},"source":["#import os, signal\n","#os.kill(os.getpid(), signal.SIGKILL)"],"execution_count":null,"outputs":[]}]}