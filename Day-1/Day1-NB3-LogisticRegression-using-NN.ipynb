{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0FslCE7TeuuQ"},"source":["# **Deep Learning and CNN for Computer Vision, Hokkaido University**\n","\n","## Day 1, Notebook -3: Logistic Regression using Neural Network\n","\n","\n","In this session you will be implementing a Logistic Regression Classifier for Dogs and Cats classification.\n","\n","So lets get started!\n","\n","\n","## Tasks for this Notebook:\n","\n","1. Implementation of Logistic Regression classifier using Keras API.\n","2. Train and test model\n","3. Using Callbacks\n"]},{"cell_type":"markdown","metadata":{"id":"rahZjYapfk0y"},"source":["### Step 1: Import required packages\n","\n","we will need tensorflow, numpy, os and keras\n"]},{"cell_type":"code","metadata":{"id":"OB0ONDu2OVEm"},"source":["## Import the required packages ##\n","import tensorflow as tf\n","import os\n","\n","import math, numpy as np\n","import sklearn.datasets\n","import matplotlib.pyplot as plt\n","\n","import os\n","import h5py\n","import glob\n","import cv2\n","\n","from tensorflow.keras.preprocessing import image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mAbw57XVOsnJ"},"source":["from tensorflow import keras\n","print(tf.__version__)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g1gJI8OsOuZQ"},"source":["### Step 2: Dataset Preparation, Dogs and Cats"]},{"cell_type":"code","source":["# Connect Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"QpQKd7UC1rGO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Unzip the Dogs and Cats zip file\n","\n","%cd \"/content/gdrive/MyDrive/Hokkaido Uni DL/Day-1\"\n","!unzip Cats-Dogs-dataset-64.zip"],"metadata":{"id":"yDd5Gsm-1ZgI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"id":"bdhe5krz1o1n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Helper function to read the Cats and Dogs dataset and return the train and test splits (Don't modify)"],"metadata":{"id":"zHjYGrk0-tAi"}},{"cell_type":"code","metadata":{"id":"Ry-kriKKKCKB"},"source":["def loadDataset():\n","\n","  ## Read all the train and test images and flatten them for training and testing\n","  train_path   = \"./TrainData\"\n","  test_path    = \"./TestData\"\n","  train_labels = os.listdir(train_path)\n","  test_labels  = os.listdir(test_path)\n","\n","  image_size       = (64, 64)\n","  num_train_images = 200\n","  num_test_images  = 100\n","  num_channels     = 3\n","\n","  train_x = np.zeros(((image_size[0]*image_size[1]*num_channels), num_train_images))\n","  train_y = np.zeros((1, num_train_images))\n","  test_x  = np.zeros(((image_size[0]*image_size[1]*num_channels), num_test_images))\n","  test_y  = np.zeros((1, num_test_images))\n","\n","  #----------------\n","  # TRAIN dataset\n","  #----------------\n","  count = 0\n","  num_label = 0\n","  for i, label in enumerate(train_labels):\n","    cur_path = train_path + \"/\" + label\n","    #print(glob.glob(cur_path + \"/*.jpg\"))\n","    for image_path in glob.glob(cur_path + \"/*.jpg\"):\n","      img = image.load_img(image_path, target_size=image_size)\n","      #print(image_path)\n","      x   = image.img_to_array(img)\n","      x   = x.flatten()\n","      x   = np.expand_dims(x, axis=0)\n","      train_x[:,count] = x\n","      train_y[:,count] = num_label\n","      count += 1\n","      #Read only 100 samples for each class for training\n","      if (count==99 or count==199):\n","        break\n","    num_label += 1\n","\n","  #--------------\n","  # TEST dataset\n","  #--------------\n","  count = 0\n","  num_label = 0\n","  for i, label in enumerate(test_labels):\n","    cur_path = test_path + \"/\" + label\n","    for image_path in glob.glob(cur_path + \"/*.jpg\"):\n","      img = image.load_img(image_path, target_size=image_size)\n","      x   = image.img_to_array(img)\n","      x   = x.flatten()\n","      x   = np.expand_dims(x, axis=0)\n","      test_x[:,count] = x\n","      test_y[:,count] = num_label\n","      count += 1\n","    num_label += 1\n","\n","  #------------------\n","  # standardization\n","  #------------------\n","  train_x = train_x/255.\n","  test_x  = test_x/255.\n","\n","\n","  ## Print the statistics of the data\n","  print (\"train_labels : \" + str(train_labels))\n","  print (\"train_x shape: \" + str(train_x.shape))\n","  print (\"train_y shape: \" + str(train_y.shape))\n","  print (\"test_x shape : \" + str(test_x.shape))\n","  print (\"test_y shape : \" + str(test_y.shape))\n","\n","  #-----------------\n","  # save using h5py\n","  #-----------------\n","  h5_train = h5py.File(\"train_x.h5\", 'w')\n","  h5_train.create_dataset(\"data_train\", data=np.array(train_x))\n","  h5_train.close()\n","\n","  h5_test = h5py.File(\"test_x.h5\", 'w')\n","  h5_test.create_dataset(\"data_test\", data=np.array(test_x))\n","  h5_test.close()\n","\n","  return train_x, train_y, test_x, test_y\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Read the Cats and Dogs dataset\n","train_set_x, train_set_y, test_set_x, test_set_y = loadDataset()"],"metadata":{"id":"eMEfdkSl108K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check whats inside the 'Y' matrix/array\n","test_set_y"],"metadata":{"id":"12AAuUw56efe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qp3nej13Pa0G"},"source":["### Step 3:  Design the Logistic Regression classifier"]},{"cell_type":"code","source":["model = keras.Sequential([\n","    keras.layers.Flatten(input_shape=(64, 64, 3)),\n","    keras.layers.Dense(1, activation='sigmoid')\n","])"],"metadata":{"id":"aOCp6PxS4sI9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Notes:**\n","* **Sequential model.** This is the simplest kind of Keras model, for neural networks which defines a SEQUENCE of layers.\n","\n","* **Flatten.** Flatten just takes that image and turns it into a 1-dimensional vector.\n","\n","* Next we add a Dense hidden layer with 1 neuron, also using the **Sigmoid** activation function.  \n","* **Dense.** Adds a layer to the neural network which is followed by activation function of Sigmoid.\n"],"metadata":{"id":"8x2F3ygQ_70F"}},{"cell_type":"markdown","source":["## Step 4: Compile the model: add loss function, optimizer etc."],"metadata":{"id":"l16oGPxTEy7m"}},{"cell_type":"code","source":["# Compile the model: Add loss function, Optimizer, and evaluation metrics\n","model.compile(optimizer= tf.keras.optimizers.SGD(learning_rate=0.01), # Adjust learning rate as needed\n","              loss='binary_crossentropy', # Binary cross-entropy is negative log-likelihood for binary classification\n","              metrics=['accuracy'])\n"],"metadata":{"id":"3UdipTtx_aZR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Notes:**\n","This code snippet configures the training process for a logistic regression model implemented using TensorFlow/Keras. Let's break down each part:\n","\n","**`model.compile(...)`**\n","\n","This line compiles the Keras model, preparing it for training.  Here's a breakdown of the arguments:\n","\n","* **`optimizer=tf.keras.optimizers.SGD(learning_rate=0.01)`**: This specifies the optimization algorithm used to update the model's weights during training.  Here, Stochastic Gradient Descent (SGD) is chosen with a learning rate of 0.01.\n","\n","    * **Optimizer:** The optimization algorithm is crucial for finding the best model weights.  SGD iteratively adjusts weights to minimize the loss function.  Other optimizers like Adam or RMSprop are often preferred over SGD in practice, as they tend to converge faster and more reliably.\n","    * **Learning Rate (0.01):** This controls the step size during weight updates.  A smaller learning rate leads to slower but potentially more stable convergence, while a larger learning rate can lead to faster convergence but may overshoot the optimal weights or fail to converge.  The optimal learning rate is problem-dependent and often needs tuning.\n","\n","\n","* **`loss='binary_crossentropy'`**: This sets the loss function, which measures the difference between the model's predictions and the true labels.  'binary_crossentropy' is appropriate for binary classification problems (two classes, in this case, cats and dogs).  It calculates the negative log-likelihood of the predicted probabilities given the true labels.  This loss function penalizes the model more for incorrect predictions with high confidence.  For multi-class classification, other loss functions like 'categorical_crossentropy' would be used.\n","\n","* **`metrics=['accuracy']`**: This defines the metrics used to evaluate the model's performance during training and testing. Here, accuracy (the percentage of correctly classified samples) is used.  You could include other metrics like precision, recall, or F1-score for a more comprehensive evaluation.\n","\n","**In summary:** This `model.compile()` line sets up the model for training by defining how the model's weights will be adjusted (SGD with a learning rate of 0.01), how the model's performance will be measured (binary cross-entropy loss), and what metrics will be tracked during training (accuracy in this case).  These settings are crucial for effective training and depend on the nature of the problem and the dataset.\n"],"metadata":{"id":"m4RiiuNQArUH"}},{"cell_type":"markdown","source":["## Step 5: Train the model"],"metadata":{"id":"0Lo1IpEdFBep"}},{"cell_type":"code","source":["# Train the model\n","H = model.fit(train_set_x.T.reshape(-1, 64, 64, 3), train_set_y.T, epochs=100)"],"metadata":{"id":"qeVM0-LP_rG0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 6: Evaluate the model on test dataset and test on single image"],"metadata":{"id":"VJOjMr7jFKmC"}},{"cell_type":"code","source":["# Evaluate the model on the test dataset\n","loss, accuracy = model.evaluate(test_set_x.T.reshape(-1, 64, 64, 3), test_set_y.T, verbose=0)\n","print(f\"Test Loss: {loss:.4f}\")\n","print(f\"Test Accuracy: {accuracy:.4f}\")\n","\n","# Test a single image\n","def predict_single_image(image_path):\n","  img = image.load_img(image_path, target_size=(64, 64))\n","  x = image.img_to_array(img)\n","  x = np.expand_dims(x, axis=0)\n","  x = x / 255.0  # Normalize the image\n","  prediction = model.predict(x)\n","  predicted_class = 1 if prediction[0][0] > 0.5 else 0\n","  print(f\"Prediction: {predicted_class}\")\n","  return predicted_class\n","\n","# Example usage\n","image_path = \"/content/gdrive/MyDrive/Hokkaido Uni DL/Day-1/TestData/cat/cat.282.jpg\" # Replace with actual path to your test image\n","predicted_class = predict_single_image(image_path)"],"metadata":{"id":"k9ResdVe695E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualize the model and Model Summary"],"metadata":{"id":"KVUXbY14vrCG"}},{"cell_type":"code","source":["#Visualize Network Model\n","modelViz_file= 'model_1.png' # Model filename\n","\n","# Plot model\n","keras.utils.plot_model(model, to_file=modelViz_file, show_shapes=True)"],"metadata":{"id":"7ZHVRkW5sGPI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model Summary\n","model.summary()"],"metadata":{"id":"bl_99RXYDNa-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e0ZtagYTglaq"},"source":["## Plot the loss curve"]},{"cell_type":"code","metadata":{"id":"Wo4kRH1Lg7xF"},"source":["type(H)\n","print(H.history.keys())"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the loss curve\n","plt.plot(H.history['loss'])\n","plt.title('Model Loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.show()"],"metadata":{"id":"dQgnDxamDbaC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r5FdZG-Gkz5M"},"source":["## Using Callbacks for early stopping training"]},{"cell_type":"code","metadata":{"id":"pm-WwSeu2HGh"},"source":["# Define Callback and stopping criteria\n","class myCallback(tf.keras.callbacks.Callback):\n","  def on_epoch_end(self, epoch, logs={}):\n","    if(logs.get('loss')<0.1): #Stopping criteria\n","      print(\"\\nReached required accuracy so cancelling training!\")\n","      self.model.stop_training = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6pBXByWP2-G-"},"source":["# Design the model\n","callbacks = myCallback()\n","model = keras.Sequential([\n","    keras.layers.Flatten(input_shape=(64, 64, 3)),\n","    keras.layers.Dense(1, activation='sigmoid')\n","])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compile the model: add loss function, optimizer etc.\n","model.compile(optimizer= tf.keras.optimizers.SGD(learning_rate=0.01), # Adjust learning rate as needed\n","              loss='binary_crossentropy', # Binary cross-entropy is negative log-likelihood for binary classification\n","              metrics=['accuracy'])"],"metadata":{"id":"-7q6eKYrqbf2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model with Callback\n","H = model.fit(train_set_x.T.reshape(-1, 64, 64, 3), train_set_y.T, epochs=1000, callbacks=[callbacks])"],"metadata":{"id":"1jBqXZQ6D7PQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the loss curve\n","plt.plot(H.history['loss'])\n","plt.title('Model Loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.show()"],"metadata":{"id":"bpGqJwUeqonl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vGP7sRzZHPVX"},"execution_count":null,"outputs":[]}]}